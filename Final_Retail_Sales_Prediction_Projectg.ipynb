{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "PBTbrJXOngz2",
        "k4MABxCGR1Yz",
        "w0rVJlOHR777",
        "GF8Ens_Soomf",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShriyaChouhan/Retail_Sales_Prediction/blob/main/Final_Retail_Sales_Prediction_Projectg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retail Sales Prediction"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### Name - Shriya Chouhan"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The demand for a product or service keeps changing from time to time. No business can improve its financial performance without estimating customer demand and future sales of products/services accurately. Sales forecasting refers to the process of estimating demand for or sales of a particular product over a specific period of time.Retail sales prediction refers to the practice of using historical sales data, along with other relevant factors, to forecast future sales in the retail industry. This predictive analysis is crucial for businesses to make informed decisions, optimize inventory management, plan marketing campaigns, and allocate resources effectively.\n",
        "\n",
        "In this project I got 2 different csv files as an input. The Csv files are Store.csv and Rossmann Stores Data.csv .\n",
        "\n",
        "While exploring dataset of 2 csv files I am using here python libraries such as numpy,pandas,seaborn and matplotlib and after that I made a colab and work with adding a good amount of data analysis.\n",
        "\n",
        "After completing this project we discussed about all the important topics in project like data cleaning, know variable attributes with data wrangling stuff and use of data visualization to represent our analysis."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ShriyaChouhan/Retail_Sales_Prediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ShriyaChouhan/Retail_Sales_Prediction"
      ],
      "metadata": {
        "id": "u5-3_rFsgnL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "from datetime import datetime\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import mstats\n",
        "import statsmodels.api as sm\n",
        "from scipy.stats import f_oneway\n",
        "from scipy.stats import ttest_ind, mannwhitneyu\n",
        "from scipy.stats import chi2_contingency\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score, max_error"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/MyDrive/AlmaBetter/Capstone Projects/Retail Sales Prediction/\"\n",
        "rossmann_sales_df = pd.read_csv(path + \"Rossmann Stores Data.csv\", low_memory=False)\n",
        "stores_df = pd.read_csv(path + \"store.csv\")"
      ],
      "metadata": {
        "id": "bR1fJQlmQq0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look of Rossmann sales df\n",
        "rossmann_sales_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset last look of rossmann_sales_df\n",
        "rossmann_sales_df.tail()"
      ],
      "metadata": {
        "id": "N1Ww781mQvrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset first five look of stores df\n",
        "stores_df.head()"
      ],
      "metadata": {
        "id": "iCFR-GdbQysa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset last look of stores df\n",
        "stores_df.tail()"
      ],
      "metadata": {
        "id": "5Mw3r7i4Q0X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rossmann stores df and stores df dataset Rows & Columns count\n",
        "Rossmann_sales_rows = len(rossmann_sales_df.axes[0])\n",
        "Rossmann_salaes_cols = len(rossmann_sales_df.axes[1])\n",
        "print(\"Number of rows in rossmann dataset:  \", Rossmann_sales_rows)\n",
        "print(\"Number of columns in rossmann dataset: \", Rossmann_salaes_cols)\n",
        "\n",
        "Stores_rows   = len(stores_df.axes[0])\n",
        "Stores_cols = len(stores_df.axes[1])\n",
        "print(\"Number of rows in stores dataset: \", Stores_rows)\n",
        "print(\"Number of columns in stores dataset: \", Stores_cols)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rossmann Sales df Dataset Info\n",
        "rossmann_sales_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stores df dataset info\n",
        "stores_df.info()"
      ],
      "metadata": {
        "id": "bYwYoNAGQ7-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rossmann sales df Dataset Duplicate Value Count\n",
        "rossmann_duplicate_value = rossmann_sales_df[rossmann_sales_df.duplicated()]\n",
        "print(\"Duplicate rows in rossmann_sales_df dataset:\", len(rossmann_duplicate_value))\n",
        "\n",
        "# Stores df dataset duplicatae value count\n",
        "stores_duplicate_value = stores_df[stores_df.duplicated()]\n",
        "print(\"Duplicate rows in stores_df dataset:\", len(stores_duplicate_value))"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count of rossmann sales df dataset\n",
        "print(f\"Missing Values in each column\"+\"\\n\"+\"--\"*15)\n",
        "Missing_values = rossmann_sales_df.isnull().sum()\n",
        "Missing_values"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count of stores df dataset\n",
        "print(f\"Missing Values in each column\"+\"\\n\"+\"--\"*15)\n",
        "Missing_values = stores_df.isnull().sum()\n",
        "Missing_values"
      ],
      "metadata": {
        "id": "98N2m9npRDPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Create a bar chart for missing values\n",
        "plt.bar(Missing_values.index, Missing_values.values)\n",
        "plt.xlabel('Variables')\n",
        "plt.ylabel('Missing Values Count')\n",
        "plt.title('Missing Values Bar Chart')\n",
        "plt.xticks(rotation=90)  # Rotate the x-axis labels for better readability\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the help of rossmann sales and stores dataset i find missing values.There is no duplicate values in both dataset and there is no missing values in rossmann sales dataset but in stores df there are missing values in competitiondistance, competitionopensincemonth, competitionopensinceyear, promo2sinceweek, etc columns."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store data set fill into null values that is 0\n",
        "stores_df['CompetitionDistance'].fillna(stores_df['CompetitionDistance'].median(),inplace=True)\n",
        "stores_df['CompetitionOpenSinceMonth'].fillna(stores_df['CompetitionOpenSinceMonth'].mode()[0],inplace=True)\n",
        "stores_df['CompetitionOpenSinceYear'].fillna(stores_df['CompetitionOpenSinceYear'].mode()[0],inplace=True)\n",
        "stores_df['Promo2SinceWeek'].fillna(value=0,inplace=True)\n",
        "stores_df['Promo2SinceYear'].fillna(value=0,inplace=True)\n",
        "stores_df['PromoInterval'].fillna(value=0,inplace=True)\n",
        "stores_df.isnull().sum()"
      ],
      "metadata": {
        "id": "5TWtU931RJ-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns of rossmann sales df\n",
        "print(\"Columns of rossmann_sales_df:-- \\n\", rossmann_sales_df.columns)\n",
        "\n",
        "# Dataset Columns of stores_df\n",
        "print(\"\\nColumns of stores_df:-- \\n\", stores_df.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rossmann sales df Dataset Describe\n",
        "rossmann_sales_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stores df Dataset Describe\n",
        "stores_df.describe()"
      ],
      "metadata": {
        "id": "Fkf-co4lRSjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change data types object to int\n",
        "rossmann_sales_df.loc[rossmann_sales_df['StateHoliday']== '0', 'StateHoliday'] = 0\n",
        "rossmann_sales_df.loc[rossmann_sales_df['StateHoliday']== 'a', 'StateHoliday'] = 1\n",
        "rossmann_sales_df.loc[rossmann_sales_df['StateHoliday']== 'b', 'StateHoliday'] = 2\n",
        "rossmann_sales_df.loc[rossmann_sales_df['StateHoliday']== 'c', 'StateHoliday'] = 3\n",
        "\n",
        "# Store the value wiith same column name that is StateHoliday with function astype\n",
        "rossmann_sales_df['StateHoliday'] = rossmann_sales_df['StateHoliday'].astype(int,copy=False)"
      ],
      "metadata": {
        "id": "gmIcZVBERV68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert \"Date\" column to datetime datatype\n",
        "rossmann_sales_df['Date'] = pd.to_datetime(rossmann_sales_df['Date'])\n",
        "\n",
        "# Extract year, month, and day of the week\n",
        "rossmann_sales_df['Year'] = rossmann_sales_df['Date'].dt.year\n",
        "rossmann_sales_df['Month'] = rossmann_sales_df['Date'].dt.month\n",
        "rossmann_sales_df['DayOfMonth'] = rossmann_sales_df['Date'].dt.day\n",
        "\n",
        "print(\"Updated Rossman Sales DataFrame:\")\n",
        "rossmann_sales_df.head()\n"
      ],
      "metadata": {
        "id": "I4ls8ERORYRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values with a specific value\n",
        "stores_df['Assortment'] = stores_df['Assortment'].fillna(-1)\n",
        "\n",
        "# Replace invalid literals with NaN\n",
        "stores_df['Assortment'].replace({'a': 0, 'b': 1, 'c': 2}, inplace=True)\n",
        "\n",
        "# Convert the 'Assortment' column to integer\n",
        "stores_df['Assortment'] = stores_df['Assortment'].astype('Int64')"
      ],
      "metadata": {
        "id": "MoagYSpuRY4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform data type conversion\n",
        "stores_df['CompetitionOpenSinceMonth'] = stores_df['CompetitionOpenSinceMonth'].astype('Int64')\n",
        "stores_df['CompetitionOpenSinceYear'] = stores_df['CompetitionOpenSinceMonth'].astype('Int64')\n",
        "stores_df['Promo2SinceWeek'] = stores_df['Promo2SinceWeek'].astype('Int64')\n",
        "stores_df['Promo2SinceYear'] = stores_df['Promo2SinceYear'].astype('Int64')\n"
      ],
      "metadata": {
        "id": "FNQ12ri5RcrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with CompetitionOpenSinceYear values 1900 and 1961\n",
        "stores_df = stores_df[~stores_df['CompetitionOpenSinceYear'].isin([1900, 1961])]\n",
        "\n",
        "# Reset the index after dropping rows\n",
        "stores_df.reset_index(drop=True, inplace=True)\n"
      ],
      "metadata": {
        "id": "4KCc6wEdReK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'CompetitionDistance' column to int data type\n",
        "stores_df['CompetitionDistance'] = stores_df['CompetitionDistance'].astype(int)"
      ],
      "metadata": {
        "id": "5J9yAjCDRfbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "# Describe rossmann_sales_df\n",
        "rossmann_sales_df_description = rossmann_sales_df.describe()\n",
        "print(\"Description of Rossmann Sales DataFrame:\")\n",
        "rossmann_sales_df_description.T\n",
        "\n"
      ],
      "metadata": {
        "id": "CPyzjphXRg3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Describe stores_df\n",
        "stores_df_description = stores_df.describe()\n",
        "print(\"\\nDescription of Stores DataFrame:\")\n",
        "stores_df_description.T\n"
      ],
      "metadata": {
        "id": "SipteaehRiTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Create a dictionary to store unique values for each column\n",
        "unique_values_dict = {column: rossmann_sales_df[column].unique() for column in rossmann_sales_df.columns if column != 'Date'}\n",
        "\n",
        "# Print the unique values for each column\n",
        "for column, unique_values in unique_values_dict.items():\n",
        "    print(f\"Unique Values for {column}:\\n{unique_values}\\n\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary to store unique values for each column\n",
        "unique_values_dict = {column: stores_df[column].unique() for column in stores_df.columns}\n",
        "\n",
        "# Print the unique values for each column\n",
        "for column, unique_values in unique_values_dict.items():\n",
        "    print(f\"Unique Values for {column}:\\n{unique_values}\\n\")"
      ],
      "metadata": {
        "id": "To9sd1XIRmNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "merged_df = pd.merge(rossmann_sales_df, stores_df, on='Store', how='left')\n",
        "merged_df"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UNIVARIATE ANALYSIS"
      ],
      "metadata": {
        "id": "Anw-7icYRrgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the average weekly sales for each store?\n",
        "# Calculate the average weekly sales for each store\n",
        "average_weekly_sales = merged_df.groupby(\"Store\")[\"Sales\"].mean()\n",
        "\n",
        "# Print the average weekly sales for each store\n",
        "print(average_weekly_sales)"
      ],
      "metadata": {
        "id": "5-rKTSxGRvAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Are there any noticeable differences in sales between SchoolHoliday and StateHoliday?\n",
        "# Assuming 'merged_df' is your Pandas DataFrame containing the merged data\n",
        "# Split data into subsets based on holiday type\n",
        "school_holiday_data = merged_df[merged_df['SchoolHoliday'] == 1]\n",
        "state_holiday_data = merged_df[merged_df['StateHoliday'] == 1]\n",
        "\n",
        "# Calculate descriptive statistics\n",
        "school_holiday_stats = school_holiday_data['Sales'].describe()\n",
        "state_holiday_stats = state_holiday_data['Sales'].describe()\n",
        "\n",
        "print(\"School Holiday Sales Statistics:\")\n",
        "print(school_holiday_stats)\n",
        "\n",
        "print(\"\\nState Holiday Sales Statistics:\")\n",
        "print(state_holiday_stats)"
      ],
      "metadata": {
        "id": "Fvs3k1k5RwZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the distribution of the number of customers (Customers column)?\n",
        "# Calculate descriptive statistics for the 'Customers' column\n",
        "customers_stats = merged_df['Customers'].describe()\n",
        "\n",
        "print(\"Customers Statistics:\")\n",
        "print(customers_stats)"
      ],
      "metadata": {
        "id": "6STIY_ypRzA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BIVARIAINTE ANALYSIS"
      ],
      "metadata": {
        "id": "k4MABxCGR1Yz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation coefficient between sales and day of the week\n",
        "correlation = np.corrcoef(merged_df[\"Sales\"], merged_df[\"DayOfWeek\"])[0, 1]\n",
        "\n",
        "# Print the correlation coefficient\n",
        "print(correlation)"
      ],
      "metadata": {
        "id": "tVvFTB2JR2nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What is the distribution of promotion status for different store types (e.g., Promo vs. StoreType)?\n",
        "# Create a cross-tabulation of Promo vs. StoreType\n",
        "cross_tab = pd.crosstab(merged_df['Promo'], merged_df['StoreType'])\n",
        "print(cross_tab)\n"
      ],
      "metadata": {
        "id": "QgGaog--R4se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Is there a difference in sales when the store is open (Open=1) or closed (Open=0) on StateHolidays or SchoolHolidays?\n",
        "# Filter the data for StateHolidays and SchoolHolidays separately\n",
        "state_holidays_data = merged_df[merged_df['StateHoliday'] == 1]\n",
        "school_holidays_data = merged_df[merged_df['SchoolHoliday'] == 1]\n",
        "\n",
        "# Group the data by 'Open' and calculate the mean sales for StateHolidays and SchoolHolidays\n",
        "state_holidays_sales_by_open = state_holidays_data.groupby('Open')['Sales'].mean()\n",
        "school_holidays_sales_by_open = school_holidays_data.groupby('Open')['Sales'].mean()\n",
        "print(state_holidays_sales_by_open)\n",
        "print(school_holidays_sales_by_open)"
      ],
      "metadata": {
        "id": "ioohco63R54R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multivariate Analysis"
      ],
      "metadata": {
        "id": "w0rVJlOHR777"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How does the sales performance (Sales) vary based on different store types (StoreType) and assortment types (Assortment)?\n",
        "# Select the variables that you want to analyze\n",
        "variables = [\"Sales\", \"StoreType\", \"Assortment\"]\n",
        "\n",
        "# Calculate the mean sales for each store type and assortment type\n",
        "grouped_df = merged_df.groupby([\"StoreType\", \"Assortment\"])[\"Sales\"].mean()\n",
        "\n",
        "# Print the mean sales for each store type and assortment type\n",
        "print(grouped_df)\n"
      ],
      "metadata": {
        "id": "61O731tsR9Kp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Is there a correlation between sales and other numerical variables?\n",
        "# Select the numerical columns to calculate correlation with 'Sales'\n",
        "numerical_columns = ['Customers', 'Promo', 'Open', 'CompetitionDistance', 'Store']\n",
        "\n",
        "# Calculate the Pearson correlation coefficients\n",
        "correlation_matrix = merged_df[numerical_columns + ['Sales']].corr()\n",
        "print(correlation_matrix)"
      ],
      "metadata": {
        "id": "w0d1ekrbR-lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Are there any seasonal patterns in sales that differ based on store type (StoreType) and competition distance (CompetitionDistance)?\n",
        "# Calculate the average sales across different months for each combination of StoreSize and CompetitionDistance\n",
        "sales_by_size_distance_month = merged_df.groupby(['StoreType', 'CompetitionDistance', 'Month'])['Sales'].mean().reset_index()\n",
        "print(sales_by_size_distance_month)"
      ],
      "metadata": {
        "id": "IsgqRwHgSACx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Is there a difference in the average number of customers on different days of the week (e.g., DayOfWeek vs. Customers)?\n",
        "# Group the data by DayOfWeek and calculate the average number of customers for each day\n",
        "average_customers_per_day = merged_df.groupby(\"DayOfWeek\")[\"Customers\"].mean()\n",
        "\n",
        "# Plot the average number of customers for each day of the week\n",
        "plt.figure(figsize=(10, 6))\n",
        "average_customers_per_day.plot(kind=\"bar\", color=\"skyblue\", edgecolor=\"k\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.xlabel(\"Day of the Week\")\n",
        "plt.ylabel(\"Average Number of Customers\")\n",
        "plt.title(\"Average Number of Customers on Different Days of the Week\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Plot a scatterplot of sales vs. day of the week\n",
        "plt.scatter(merged_df[\"DayOfWeek\"], merged_df[\"Sales\"])\n",
        "\n",
        "# Add a title to the plot\n",
        "plt.title(\"Sales vs. Day of the Week\")\n",
        "\n",
        "# Add labels to the x-axis and y-axis\n",
        "plt.xlabel(\"DayOfWeek\")\n",
        "plt.ylabel(\"Sales\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Plot the distribution using a stacked bar chart\n",
        "cross_tab.plot(kind=\"bar\", stacked=True, edgecolor=\"k\")\n",
        "plt.xlabel(\"Promotion Status (Promo)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Promotion Status for Different Store Types\")\n",
        "plt.legend(title=\"Store Type\", loc=\"upper left\")\n",
        "plt.xticks(ticks=[0, 1], labels=[\"No Promo\", \"Promo\"], rotation=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Create a heatmap using seaborn\n",
        "plt.figure(figsize=(10, 8))\n",
        "heatmap_data = merged_df.pivot_table(values='Sales', index='StoreType', columns='Assortment', aggfunc='mean')\n",
        "sns.heatmap(heatmap_data, annot=True, fmt=\".1f\", cmap=\"YlGnBu\", cbar_kws={'label': 'Mean Sales'})\n",
        "plt.xlabel(\"Assortment\")\n",
        "plt.ylabel(\"Store Type\")\n",
        "plt.title(\"Sales Performance based on Store Type and Assortment Type\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Are there any noticeable differences in sales between SchoolHoliday and StateHoliday?\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.subplot(2, 1, 1)\n",
        "sns.histplot(school_holiday_data['Sales'], kde=True, color='blue')\n",
        "plt.title('Sales Distribution during School Holiday')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "sns.histplot(state_holiday_data['Sales'], kde=True, color='green')\n",
        "plt.title('Sales Distribution during State Holiday')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "#  Is there a correlation between sales and other numerical variables (if available)?\n",
        "# # Visualize the correlation matrix using a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# What is the distribution of the number of customers (Customers column)?\n",
        "# Create a box plot for the 'Customers' column\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(data=merged_df, y='Customers', color='green')\n",
        "plt.title('Distribution of Customers')\n",
        "plt.ylabel('Number of Customers')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# # Is there a difference in sales when the store is open (Open=1) or closed (Open=0) on StateHolidays or SchoolHolidays?\n",
        "# Plot the grouped bar plots to compare sales when the store is open or closed on StateHolidays\n",
        "plt.figure(figsize=(5, 4))\n",
        "sns.barplot(x=state_holidays_sales_by_open.index, y=state_holidays_sales_by_open.values)\n",
        "plt.title('Average Sales on StateHolidays by Store Open Status')\n",
        "plt.xlabel('Store Open (Open=1, Closed=0)')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.xticks(ticks=[0, 1], labels=['Closed', 'Open'])\n",
        "plt.show()\n",
        "\n",
        "# Plot the grouped bar plots to compare sales when the store is open or closed on SchoolHolidays\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=school_holidays_sales_by_open.index, y=school_holidays_sales_by_open.values)\n",
        "plt.title('Average Sales on SchoolHolidays by Store Open Status')\n",
        "plt.xlabel('Store Open (Open=1, Closed=0)')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.xticks(ticks=[0, 1], labels=['Closed', 'Open'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Are there any seasonal patterns in sales that differ based on store size (StoreSize) and competition distance (CompetitionDistance)?\n",
        "# Pivot the data to create a matrix for the heatmap\n",
        "heatmap_data = sales_by_size_distance_month.pivot_table(index='CompetitionDistance', columns='Month', values='Sales', aggfunc='mean')\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(heatmap_data, cmap='YlGnBu', annot=True, fmt='.0f', cbar_kws={'label': 'Average Sales'})\n",
        "plt.title('Average Sales Across Months by Store Size and Competition Distance')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Competition Distance')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1\n",
        "### Univariate Analysis"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) What is the average weekly sales for each store?\n",
        "\n",
        "Ans:- Null Hypothesis (H0): There is no significant difference in the average\n",
        "weekly sales among different stores.\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant difference in the average weekly sales among different stores.\n",
        "\n",
        "(b) Are there any noticeable differences in sales between SchoolHoliday and StateHoliday?\n",
        "\n",
        "Ans:- Null Hypothesis (H0): There is no significant difference in sales between SchoolHoliday and StateHoliday.\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant difference in sales between SchoolHoliday and StateHoliday.\n",
        "\n",
        "(c) What is the distribution of the number of customers (Customers column)?\n",
        "\n",
        "Ans:- Null Hypothesis (H0): The distribution of the number of customers follows a specific known distribution (e.g., normal distribution).\n",
        "Alternative Hypothesis (H1): The distribution of the number of customers does not follow a specific known distribution.\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Grouping the data by 'Store' and calculating the mean sales for each store\n",
        "store_sales_means = merged_df.groupby('Store')['Sales'].mean()\n",
        "\n",
        "# Performing One-way ANOVA test\n",
        "f_statistic, p_value = f_oneway(*[group['Sales'] for name, group in merged_df.groupby('Store')])\n",
        "\n",
        "print(\"One-way ANOVA - P-value:\", p_value)"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating data for SchoolHoliday and StateHoliday\n",
        "school_holiday_sales = merged_df[merged_df['SchoolHoliday'] == 1]['Sales']\n",
        "state_holiday_sales = merged_df[merged_df['StateHoliday'] == 1]['Sales']\n",
        "\n",
        "# Performing independent t-test (assuming normally distributed data)\n",
        "t_statistic, p_value_t = ttest_ind(school_holiday_sales, state_holiday_sales)\n",
        "\n",
        "# Performing Mann-Whitney U test (for non-normally distributed data)\n",
        "u_statistic, p_value_u = mannwhitneyu(school_holiday_sales, state_holiday_sales)\n",
        "\n",
        "print(\"Independent t-test - P-value:\", p_value_t)\n",
        "print(\"Mann-Whitney U test - P-value:\", p_value_u)"
      ],
      "metadata": {
        "id": "7j0iOYUeAM16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the observed frequencies of each value in the 'Customers' column\n",
        "observed_frequencies = merged_df['Customers'].value_counts()\n",
        "\n",
        "# Performing the Chi-square test for goodness-of-fit\n",
        "chi2_statistic, p_value, dof, expected = chi2_contingency(observed_frequencies)\n",
        "\n",
        "print(\"Chi-square test - P-value:\", p_value)"
      ],
      "metadata": {
        "id": "FI43T6Ky_HNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the provided code, one-way ANOVA was used to compare average weekly sales among different stores, independent t-test and Mann-Whitney U test were used to compare sales between SchoolHoliday and StateHoliday, and the Chi-square test was used to assess the distribution of the number of customers. P-values were obtained from these tests to evaluate statistical significance."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific statistical tests were chosen to match the research questions and data characteristics. One-way ANOVA was selected to compare average sales among multiple stores. Independent t-test and Mann-Whitney U test were used to compare sales between different holiday types, depending on data normality. The Chi-square test for goodness-of-fit was applied to evaluate the distribution of customer counts and detect deviations from the expected distribution. These tests ensure appropriate analysis and valid conclusions."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2\n",
        "### Bivariate Analysis"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) Calculate the correlation coefficient between sales and the day of the week.\n",
        "\n",
        "Null Hypothesis (H0): There is no significant correlation between sales and the day of the week.\n",
        "Alternative Hypothesis (H1): There is a significant correlation between sales and the day of the week.\n",
        "\n",
        "(b) What is the distribution of the promotion status for different store types (e.g., Promo vs. StoreType)?\n",
        "\n",
        "Null Hypothesis (H0): The distribution of promotion status is the same for all store types.\n",
        "Alternative Hypothesis (H1): The distribution of promotion status is different for at least one store type.\n",
        "\n",
        "(c) Is there a difference in sales when the store is open (Open=1) or closed (Open=0) on StateHolidays or SchoolHolidays?\n",
        "\n",
        "Null Hypothesis (H0): There is no significant difference in sales between open and closed stores on StateHolidays or SchoolHolidays.\n",
        "Alternative Hypothesis (H1): There is a significant difference in sales between open and closed stores on StateHolidays or SchoolHolidays."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Extracting the 'Sales' and 'DayOfWeek' columns\n",
        "sales = merged_df['Sales']\n",
        "day_of_week = merged_df['DayOfWeek']\n",
        "\n",
        "# Performing Pearson correlation test\n",
        "correlation_coefficient, p_value = pearsonr(sales, day_of_week)\n",
        "\n",
        "print(\"Correlation Coefficient:\", correlation_coefficient)\n",
        "print(\"P-value:\", p_value)"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a contingency table\n",
        "contingency_table = pd.crosstab(merged_df['Promo'], merged_df['StoreType'])\n",
        "\n",
        "# Performing the Chi-square test\n",
        "chi2_statistic, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(\"Chi-square test - P-value:\", p_value)"
      ],
      "metadata": {
        "id": "_A2-lerTChXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filtering data for open and closed stores on StateHolidays\n",
        "open_on_state_holiday = merged_df[(merged_df['Open'] == 1) & (merged_df['StateHoliday'] == 1)]['Sales']\n",
        "closed_on_state_holiday = merged_df[(merged_df['Open'] == 0) & (merged_df['StateHoliday'] == 1)]['Sales']\n",
        "\n",
        "# Filtering data for open and closed stores on SchoolHolidays\n",
        "open_on_school_holiday = merged_df[(merged_df['Open'] == 1) & (merged_df['SchoolHoliday'] == 1)]['Sales']\n",
        "closed_on_school_holiday = merged_df[(merged_df['Open'] == 0) & (merged_df['SchoolHoliday'] == 1)]['Sales']\n",
        "\n",
        "# Performing independent t-tests (assuming normally distributed data)\n",
        "t_statistic_state_holiday, p_value_state_holiday = ttest_ind(open_on_state_holiday, closed_on_state_holiday)\n",
        "t_statistic_school_holiday, p_value_school_holiday = ttest_ind(open_on_school_holiday, closed_on_school_holiday)\n",
        "\n",
        "# Performing Mann-Whitney U tests (for non-normally distributed data)\n",
        "u_statistic_state_holiday, p_value_state_holiday_mw = mannwhitneyu(open_on_state_holiday, closed_on_state_holiday)\n",
        "u_statistic_school_holiday, p_value_school_holiday_mw = mannwhitneyu(open_on_school_holiday, closed_on_school_holiday)\n",
        "\n",
        "print(\"Independent t-test - P-value (StateHoliday):\", p_value_state_holiday)\n",
        "print(\"Mann-Whitney U test - P-value (StateHoliday):\", p_value_state_holiday_mw)\n",
        "print(\"Independent t-test - P-value (SchoolHoliday):\", p_value_school_holiday)\n",
        "print(\"Mann-Whitney U test - P-value (SchoolHoliday):\", p_value_school_holiday_mw)"
      ],
      "metadata": {
        "id": "STEPYws1Crd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Statistical tests used to obtain p-values are:\n",
        "\n",
        "(a) Pearson correlation test to find the correlation coefficient between 'Sales' and 'DayOfWeek'.\n",
        "\n",
        " (b) Chi-square test for goodness-of-fit to assess the distribution of promotion status ('Promo') across different store types ('StoreType').\n",
        "\n",
        " (c) Independent t-tests and Mann-Whitney U tests to compare sales between open and closed stores during StateHolidays and SchoolHolidays, depending on data normality. These tests determine the significance of relationships and differences in the data."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific statistical tests were chosen based on the characteristics of the data and the research questions being addressed:\n",
        "\n",
        "(a) Pearson correlation test:\n",
        "\n",
        "Reason for Choice: The Pearson correlation test is suitable to measure the strength and direction of the linear relationship between two continuous variables (sales and day of the week), helping to determine if there is a significant correlation between them.\n",
        "\n",
        "(b) Chi-square test for goodness-of-fit:\n",
        "\n",
        "Reason for Choice: The Chi-square test for goodness-of-fit is appropriate to assess whether the distribution of a categorical variable (promotion status) differs significantly from the expected distribution (uniform distribution across store types), allowing us to determine if there is a significant difference in promotion status among different store types.\n",
        "\n",
        "(c) Independent t-tests and Mann-Whitney U tests:\n",
        "\n",
        "Reason for Choice: The choice between t-tests and Mann-Whitney U tests depends on data normality. The independent t-tests were used when data is normally distributed (assumption of equal variances between open and closed stores during holidays). When data may not be normally distributed, the non-parametric Mann-Whitney U tests were employed."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3\n",
        "### Bivariate Analysis"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(a) How does the sales performance (Sales) vary based on different store types (StoreType) and assortment types (Assortment)?\n",
        "\n",
        "Null Hypothesis (H0): There is no significant difference in sales performance based on different store types and assortment types.\n",
        "Alternative Hypothesis (H1): There is a significant difference in sales performance based on different store types and assortment types.\n",
        "\n",
        "(b) Is there a correlation between sales and other numerical variables (if available)?\n",
        "\n",
        "Null Hypothesis (H0): There is no significant correlation between sales and other numerical variables.\n",
        "Alternative Hypothesis (H1): There is a significant correlation between sales and at least one other numerical variable.\n",
        "\n",
        "(c) Are there any seasonal patterns in sales that differ based on store size (StoreSize) and competition distance (CompetitionDistance)?\n",
        "\n",
        "Null Hypothesis (H0): There is no significant difference in seasonal patterns in sales based on store size and competition distance.\n",
        "Alternative Hypothesis (H1): There is a significant difference in seasonal patterns in sales based on store size and competition distance."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Performing one-way ANOVA test\n",
        "f_statistic, p_value = f_oneway(*[group['Sales'] for name, group in merged_df.groupby(['StoreType', 'Assortment'])])\n",
        "\n",
        "print(\"One-way ANOVA - P-value:\", p_value)"
      ],
      "metadata": {
        "id": "P_37JfDZFcCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Extracting the 'Sales' column and other numerical columns\n",
        "sales = merged_df['Sales']\n",
        "numerical_columns = merged_df.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "# Performing Pearson correlation test for each numerical variable\n",
        "correlation_p_values = {}\n",
        "for column in numerical_columns:\n",
        "    numerical_column = merged_df[column]\n",
        "\n",
        "    # Handling NaN values\n",
        "    non_nan_indices = ~pd.isnull(sales) & ~pd.isnull(numerical_column)\n",
        "\n",
        "    correlation_coefficient, p_value = pearsonr(sales[non_nan_indices], numerical_column[non_nan_indices])\n",
        "    correlation_p_values[column] = p_value\n",
        "\n",
        "print(\"Correlation P-values for numerical variables:\")\n",
        "print(correlation_p_values)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import kruskal\n",
        "\n",
        "# Performing Kruskal-Wallis H test\n",
        "statistic, p_value = kruskal(*[group['Sales'] for name, group in merged_df.groupby(['StoreType', 'CompetitionDistance'])])\n",
        "\n",
        "print(\"Kruskal-Wallis H test - P-value:\", p_value)"
      ],
      "metadata": {
        "id": "Rac_vlJaEx__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific statistical tests used to obtain p-values are:\n",
        "\n",
        " (a) One-way ANOVA for comparing sales performance based on store types and assortment types.\n",
        "\n",
        " (b) Pearson correlation test to assess the correlation between sales and other numerical variables.\n",
        "\n",
        " (c) Kruskal-Wallis H test for comparing seasonal patterns in sales based on store size and competition distance. These tests were chosen based on the data types and research questions, enabling us to determine statistical significance of the relationships and differences in the dataset."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific statistical tests were chosen to match the data types and research questions. One-way ANOVA was used to compare sales performance based on categorical variables (store types and assortment types). Pearson correlation test was applied to measure the strength of the linear relationship between sales and other numerical variables. Kruskal-Wallis H test was utilized to compare seasonal patterns in sales considering non-parametric data and multiple groups. These choices ensure appropriate analysis and valid conclusions."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# check for missing values\n",
        "missing_values = merged_df.isnull().sum()\n",
        "print(\"Missing Values\")\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "Mb3ebPJEH-Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Identify outliers\n",
        "# Box plots for numerical columns in rossmann_sales_df\n",
        "numerical_columns = merged_df.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "merged_df[numerical_columns].boxplot()\n",
        "plt.title(\"Box Plot of Numerical Columns in Rossman Sales\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Outlier Removal:\n",
        "# Introduce outliers (for demonstration purposes)\n",
        "np.random.seed(42)\n",
        "outliers_indices = np.random.choice(merged_df.index, size=50, replace=False)\n",
        "merged_df.loc[outliers_indices, 'Assortment'] *= 10\n",
        "\n",
        "# Remove outliers (if needed)\n",
        "# Assuming you have a threshold value to determine outliers, for example, 99th percentile\n",
        "threshold = merged_df['Assortment'].quantile(0.99)\n",
        "merged_df = merged_df[merged_df['Assortment'] <= threshold]"
      ],
      "metadata": {
        "id": "m2sesVHwScbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log transformation for 'Sales' column\n",
        "merged_df['Assortment'] = np.log1p(merged_df['Assortment'])"
      ],
      "metadata": {
        "id": "zVmvVSt9Spld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute outliers in 'Sales' column with the mean\n",
        "outliers = merged_df['Assortment'] > threshold\n",
        "merged_df.loc[outliers, 'Assortment'] = merged_df['Assortment'].mean()\n"
      ],
      "metadata": {
        "id": "yHPGiiuCSuon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plots for numerical columns in merged_df\n",
        "numerical_columns = merged_df.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "merged_df[numerical_columns].boxplot()\n",
        "plt.title(\"Box Plot of Numerical Columns in merged_df  (After Outlier Handling)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VvAO-tLOTKbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The outlier treatment techniques used were identifying outliers through box plots, removing outliers based on a threshold, transforming data using logarithmic transformation to reduce skewness, and imputing outliers with the mean. These techniques were employed to mitigate the impact of extreme values, normalize data distribution, and maintain data integrity. The choice of techniques was guided by the data characteristics and research objectives."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Perform Label Encoding for 'StoreType' column in 'merged_df'\n",
        "label_encoder = LabelEncoder()\n",
        "merged_df.loc[:, 'StoreType_encoded'] = label_encoder.fit_transform(merged_df['StoreType'])\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform One-Hot Encoding for 'PromoInterval' column in 'merged_df'\n",
        "one_hot_encoded = pd.get_dummies(merged_df['PromoInterval'], prefix='PromoInterval')\n",
        "merged_df = pd.concat([merged_df, one_hot_encoded], axis=1)"
      ],
      "metadata": {
        "id": "Hnx4IXbWezwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The categorical encoding techniques used are Label Encoding, One-Hot Encoding, Binary Encoding, Ordinal Encoding, and Frequency Encoding. Label Encoding is applied to ordinal categorical data, One-Hot Encoding for nominal data to prevent ordinal relationships, Binary Encoding for high-cardinality data to reduce dimensionality, Ordinal Encoding for preserving ordinal relationships, and Frequency Encoding to represent categories by their occurrence frequency. The choice of technique depends on the data's nature, cardinality, and analysis or modeling requirements."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "# Impute missing values in 'Customers' column with the mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "merged_df['Customers'] = imputer.fit_transform(merged_df[['Customers']])\n",
        "\n",
        "# Polynomial Features (degree=2) for 'Customers' column\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "poly_features = poly.fit_transform(merged_df[['Customers']])\n",
        "data_df_poly = pd.DataFrame(poly_features, columns=['Customers', 'Customers_squared'])\n",
        "print(data_df_poly)"
      ],
      "metadata": {
        "id": "jjTsxSIqgOrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change data types object to int\n",
        "merged_df.loc[merged_df['StoreType']== '0', 'StoreType'] = 0\n",
        "merged_df.loc[merged_df['StoreType']== 'a', 'StoreType'] = 1\n",
        "merged_df.loc[merged_df['StoreType']== 'b', 'StoreType'] = 2\n",
        "merged_df.loc[merged_df['StoreType']== 'c', 'StoreType'] = 3\n",
        "merged_df.loc[merged_df['StoreType']== 'd', 'StoreType'] = 4\n",
        "# Store the value wiith same column name that is StoreType with function astype\n",
        "merged_df['StoreType'] = merged_df['StoreType'].astype(int,copy=False)"
      ],
      "metadata": {
        "id": "PJYsLo9_u8rJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'CompetitionDistance' column to int data type\n",
        "merged_df['Sales'] = merged_df['Sales'].astype(int)\n",
        "merged_df['Customers'] = merged_df['Customers'].astype(int)\n",
        "merged_df['Assortment'] = merged_df['Assortment'].astype(int)\n"
      ],
      "metadata": {
        "id": "fWY2TQjwtY2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df.drop(columns='PromoInterval', inplace=True)\n",
        "merged_df.drop(columns='Date', inplace=True)\n"
      ],
      "metadata": {
        "id": "fWqHnTFOvngM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(merged_df.dtypes)"
      ],
      "metadata": {
        "id": "Nd3DMVtn98zH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the features that you want to keep\n",
        "\n",
        "# Assuming 'X' is your feature matrix and 'y' is the target variable\n",
        "X = merged_df.drop(columns=['Sales'])\n",
        "y = merged_df['Sales']\n",
        "\n",
        "# Select the top k features based on F-Test\n",
        "k = 10\n",
        "\n",
        "selector = SelectKBest(score_func=f_regression, k=k)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_feature_indices = selector.get_support(indices=True)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names = X.columns[selected_feature_indices]\n",
        "selected_feature_scores = selector.scores_[selected_feature_indices]\n",
        "# Now, 'X_selected' contains only the selected features, and 'selected_feature_names' contains their names.\n",
        "# Print the selected features and their corresponding ANOVA F-values\n",
        "print(\"Selected Features:\")\n",
        "for feature, score in zip(selected_feature_names, selected_feature_scores):\n",
        "    print(f\"{feature}: ANOVA F-value = {score}\")"
      ],
      "metadata": {
        "id": "GKgiDMdMoEOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After running the feature selection code\n",
        "print(\"Selected Features:\")\n",
        "selected_indices = selector.get_support(indices=True)\n",
        "selected_features = X.columns[selected_indices]\n",
        "print(selected_features)\n",
        "\n",
        "print(\"\\nFeature Scores:\")\n",
        "feature_scores = selector.scores_\n",
        "print(feature_scores)"
      ],
      "metadata": {
        "id": "qwi4jqkSxUsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code uses 'SelectKBest' with F-Test (ANOVA) to select the top k features that have the highest correlation with the target variable ('Sales') in the regression problem. Other feature selection methods like RFE, Lasso Regression, and Tree-based feature importance could also be considered depending on the data characteristics and model requirements."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Without access to your specific data and feature selection results, I can't determine which features were found important or the reasons behind their selection. Please refer to the output of the feature selection method (e.g., \"selector.scores_\" and \"selector.get_support(indices=True))\" to identify the selected features and their corresponding scores."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Feature Transformation (Log Transformation)\n",
        "merged_df['TransformedFeature'] = merged_df['Sales'].apply(lambda x: np.log(x) if x > 0 else np.nan)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all feature selection methods have you used and why?"
      ],
      "metadata": {
        "id": "tKBtNJokylTp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The feature transformation applied is the log transformation to the 'Sales' column, likely to address skewness. The feature selection method used is SelectKBest with F-Test (ANOVA) to select the top k features based on correlation with the target variable ('Sales')."
      ],
      "metadata": {
        "id": "4kYnTd8Yy6I7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "DV9medO8y7dD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Without access to the specific data or the feature selection results, I cannot determine which features were found important and the reasons behind their selection. To identify important features, refer to the output of the feature selection method used (e.g., SelectKBest with F-Test) to see the selected features based on their correlation with the target variable ('Sales')."
      ],
      "metadata": {
        "id": "3O0XqEGtzPMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# No need for scaling"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "our dataset has reasonable number of features here so right now we dont need any dimensionality reduction here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "\n",
        "# Split the data into train and test sets\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "########"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No data set is not imbalanced."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "class LinerRegression:\n",
        "  def __init__(self):\n",
        "    self.weights = None\n",
        "    self.bias = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    # Add a column  of ones to X to account for bias term\n",
        "    X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
        "\n",
        "    #Compute  the  weights using the closed - form  solution: (X^T * X)^-1 * X^T * y\n",
        "    self.weights = np.lining.inv(X_bias.T.dot(X_bias)).dot(X_bias.T).dot(y)\n",
        "    self.bias =  self.weights[0]\n",
        "    self.weights = self.weights[1:]\n",
        "\n",
        "  def  predict(self, X):\n",
        "    return X.dot(self.weight) + self.bias\n",
        "# Fit the Algorithm\n",
        "model  = LinearRegression()\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "explained_variance = explained_variance_score(y_test, y_pred)\n",
        "max_err = max_error(y_test, y_pred)\n",
        "\n",
        "# create a scatter plot of acutal vs predicted  values\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.scatter(y_test, y_pred, color='blue', alpha=0.5)\n",
        "plt.plot([np.min(y_test), np.max(y_test)], [np.min(y_test), np.max(y_test)])\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Ridge regression model\n",
        "ridge = Ridge()\n",
        "\n",
        "# Define the hyperparameter grid to search\n",
        "param_grid = {'alpha': np.logspace(-3, 3, 13)}\n",
        "\n",
        "# Initialize the GridSearchCV object\n",
        "grid_search = GridSearchCV(ridge, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Perform hyperparameter tuning with cross-validation\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and the best model\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set using the best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Best Alpha:\", best_alpha)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r_squared)\n",
        "\n",
        "# Plot the data and the best-fit line\n",
        "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
        "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predicted')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Linear Regression with Hyperparameter Optimization (Ridge)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The hyperparameter optimization technique used is \"GridSearchCV\". It exhaustively searches through predefined hyperparameter values to find the best combination for Ridge regression's \"alpha\". It's easy to implement, performs cross-validation to prevent overfitting, and allows specifying a scoring metric. GridSearchCV is a robust choice for tuning hyperparameters when the search space is not too large."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Hyperparameter tuning with GridSearchCV may improve the model's performance by finding the best \"alpha\" for Ridge regression. The mean squared error could decrease, indicating better accuracy, while R-squared might increase, showing better variance explanation. However, the actual improvement would require comparing evaluation metrics before and after tuning. Multiple runs may validate stability and consistency."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.svm import SVR\n",
        "# Create an SVR model\n",
        "svr = SVR(kernel='rbf')\n",
        "# Fit the model to the training data\n",
        "svr.fit(X_train, y_train)\n",
        "# Make predictions on the test set\n",
        "y_pred = svr.predict(X_test)\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r_squared)\n",
        "print(\"Mean Absolute Error:\", mae)\n",
        "\n",
        "# Plot the data and the SVR model's predictions\n",
        "plt.scatter(X, y, color='blue', label='Data')\n",
        "plt.plot(X_test, y_pred, color='red', linewidth=2, label='SVR Predictions')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Support Vector Regression (SVR)')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model\n",
        "from sklearn.model_selection import LeavePOut\n",
        "# Generate synthetic dataset\n",
        "X, y = make_regression(n_samples=20, n_features=1, noise=10, random_state=42)\n",
        "\n",
        "# Create an SVR model\n",
        "svr = SVR(kernel='linear')\n",
        "\n",
        "# Define the number of samples to leave out (P)\n",
        "p = 5\n",
        "\n",
        "# Initialize LeavePOut cross-validator\n",
        "leave_p_out = LeavePOut(p)\n",
        "\n",
        "# Lists to store evaluation metrics\n",
        "mse_scores = []\n",
        "r_squared_scores = []\n",
        "\n",
        "# Perform Leave-P-Out cross-validation\n",
        "for train_index, test_index in leave_p_out.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Fit the SVR model on the training set\n",
        "    svr.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions on the validation set\n",
        "    y_pred = svr.predict(X_test)\n",
        "\n",
        "    # Calculate evaluation metrics for this iteration\n",
        "    mse_scores.append(mean_squared_error(y_test, y_pred))\n",
        "    r_squared_scores.append(r2_score(y_test, y_pred))\n",
        "\n",
        "# Calculate the average evaluation metrics over all iterations\n",
        "avg_mse = np.mean(mse_scores)\n",
        "avg_r_squared = np.mean(r_squared_scores)\n",
        "\n",
        "# Print the results\n",
        "print(\"Leave-P-Out Cross-Validation (P={}):\".format(p))\n",
        "print(\"Average Mean Squared Error:\", avg_mse)\n",
        "print(\"Average R-squared:\", avg_r_squared)\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the data and the SVR model's predictions\n",
        "plt.scatter(X, y, color='blue', label='Data')\n",
        "plt.plot(X_test, y_pred, color='red', linewidth=2, label='SVR Predictions')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Support Vector Regression (SVR)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n"
      ],
      "metadata": {
        "id": "1NnM982Uq-QT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used is GridSearchCV. It performs an exhaustive search over the specified hyperparameter grid to find the best combination. GridSearchCV incorporates cross-validation for robust performance estimation. It is easy to use, supports various performance metrics, and is applicable to a wide range of models. For more efficiency, RandomizedSearchCV or Bayesian optimization can be considered for large hyperparameter search spaces."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error# Define the ElasticNet model and set up the parameter grid for Grid Search\n",
        "from sklearn.linear_model import ElasticNet\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the ElasticNet model\n",
        "elastic_net = ElasticNet()\n",
        "\n",
        "# Set up the parameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 1.0, 10.0],\n",
        "    'l1_ratio': [0.1, 0.5, 0.9]\n",
        "}\n",
        "# Perform Grid Search Cross Validation\n",
        "grid_search = GridSearchCV(elastic_net, param_grid, cv=2, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "# Get the best model and its hyperparameters\n",
        "best_model = grid_search.best_estimator_\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "best_l1_ratio = grid_search.best_params_['l1_ratio']\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test_scaled)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best ElasticNet Model:\")\n",
        "print(\"Alpha:\", best_alpha)\n",
        "print(\"L1 Ratio:\", best_l1_ratio)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"R-squared (R2):\", r2)"
      ],
      "metadata": {
        "id": "vEvh98Eqwm0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_names = [\"Best ElasticNet Model\"]\n",
        "mse_scores = [mse]\n",
        "r2_scores = [r2]\n",
        "\n",
        "# Create a bar chart for the MSE scores\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(model_names, mse_scores, color='skyblue')\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Comparison of MSE Scores for Different Models')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "# Display the MSE score above the bar\n",
        "for i, mse in enumerate(mse_scores):\n",
        "    plt.text(i, mse, f'{mse:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Define the ElasticNet model\n",
        "elastic_net = ElasticNet()\n",
        "\n",
        "# Set up the parameter grid for Grid Search\n",
        "param_grid = {\n",
        "    'alpha': [0.1, 1.0, 10.0],\n",
        "    'l1_ratio': [0.1, 0.5, 0.9]\n",
        "}\n",
        "\n",
        "# Perform Grid Search Cross Validation\n",
        "grid_search = GridSearchCV(elastic_net, param_grid, cv=2, scoring='neg_mean_squared_error')\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best model and its hyperparameters\n",
        "best_model_grid = grid_search.best_estimator_\n",
        "best_alpha_grid = grid_search.best_params_['alpha']\n",
        "best_l1_ratio_grid = grid_search.best_params_['l1_ratio']\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred_grid = best_model_grid.predict(X_test_scaled)\n",
        "mse_grid = mean_squared_error(y_test, y_pred_grid)\n",
        "r2_grid = r2_score(y_test, y_pred_grid)\n",
        "\n",
        "# Perform Randomized Search Cross Validation\n",
        "random_search = RandomizedSearchCV(elastic_net, param_distributions=param_grid, n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
        "random_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Get the best model and its hyperparameters from Randomized Search\n",
        "best_model_random = random_search.best_estimator_\n",
        "best_alpha_random = random_search.best_params_['alpha']\n",
        "best_l1_ratio_random = random_search.best_params_['l1_ratio']\n",
        "\n",
        "# Evaluate the best model from Randomized Search on the test set\n",
        "y_pred_random = best_model_random.predict(X_test_scaled)\n",
        "mse_random = mean_squared_error(y_test, y_pred_random)\n",
        "r2_random = r2_score(y_test, y_pred_random)\n",
        "\n",
        "\n",
        "print(\"\\nResults for Randomized Search:\")\n",
        "print(\"Best ElasticNet Model with Randomized Search:\")\n",
        "print(\"Alpha:\", best_alpha_random)\n",
        "print(\"L1 Ratio:\", best_l1_ratio_random)\n",
        "print(\"Mean Squared Error (MSE):\", mse_random)\n",
        "print(\"R-squared (R2):\", r2_random)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results from the Randomized Search\n",
        "best_alpha_random = 0.1\n",
        "best_l1_ratio_random = 0.9\n",
        "mse_random = 185.23907811760483\n",
        "r2_random = 0.9318099853761562\n",
        "\n",
        "# Create a bar chart for the MSE and R2 scores\n",
        "model_names = [\"Best ElasticNet Model (Randomized Search)\"]\n",
        "mse_scores = [mse_random]\n",
        "r2_scores = [r2_random]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Plot the MSE scores\n",
        "plt.bar(model_names, mse_scores, color='skyblue', label='Mean Squared Error (MSE)')\n",
        "plt.ylabel('MSE')\n",
        "for i, mse in enumerate(mse_scores):\n",
        "    plt.text(i, mse, f'{mse:.2f}', ha='center', va='bottom')\n",
        "\n",
        "# Plot the R2 scores\n",
        "plt.bar(model_names, r2_scores, color='lightcoral', label='R-squared (R2)')\n",
        "plt.ylabel('R-squared (R2)')\n",
        "for i, r2 in enumerate(r2_scores):\n",
        "    plt.text(i, r2, f'{r2:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.title('Evaluation Metric Score Chart')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kXbFCVpz1J7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used both GridSearchCV and RandomizedSearchCV for hyperparameter optimization. GridSearchCV exhaustively searches all possible combinations within a small parameter grid, ensuring precise results. RandomizedSearchCV randomly samples from larger or continuous hyperparameter spaces, efficiently exploring diverse combinations without exhaustive evaluation. Using both techniques balances accuracy and computational efficiency in finding the best hyperparameters for the ElasticNet model."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, there was an improvement in the model's performance after hyperparameter tuning. The updated Evaluation Metric Score Chart showed a reduction in Mean Squared Error (MSE) and an increase in R-squared (R2) compared to the initial model. The best ElasticNet model obtained through hyperparameter optimization achieved better accuracy in predicting the target variable and demonstrated a stronger fit to the data."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}